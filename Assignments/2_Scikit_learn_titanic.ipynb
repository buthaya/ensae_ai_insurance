{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Hands-on - scikit learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this notebook is get hands on with Scikit-learn and demonstrate their usage in preprocessing and modeling tasks using the Titanic dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reminder from 1_Pandas\n",
    "\n",
    "The Titanic dataset contains information about passengers aboard the Titanic, including features such as age, sex, ticket class, and whether they survived the disaster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd  \n",
    "from sklearn.model_selection import train_test_split, GridSearchCV  \n",
    "from sklearn.impute import SimpleImputer  \n",
    "from sklearn.preprocessing import OneHotEncoder  \n",
    "from sklearn.compose import ColumnTransformer  \n",
    "from sklearn.pipeline import Pipeline  \n",
    "from sklearn.ensemble import RandomForestClassifier  \n",
    "from sklearn.metrics import accuracy_score, classification_report  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Data Loading and Exploration:**\n",
    "   - Load the Titanic dataset into a pandas DataFrame.\n",
    "   - Display basic information about the dataset (e.g., data types, missing values, summary statistics).\n",
    "   - Explore the distribution of the target variable (`Survived`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1: Data Loading and Exploration\n",
    "# Load the Titanic dataset\n",
    "titanic_df = # read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the distribution of the target variable (Survived)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Data Preprocessing:**\n",
    "   - Handle missing values in the dataset (e.g., drop unnecessary columns, impute missing values).\n",
    "   - Encode categorical variables using one-hot encoding.\n",
    "   - Split the dataset into features (X) and target variable (y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first rows\n",
    "titanic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns that you want to impute and encode  \n",
    "num_features = # list of numerical column names\n",
    "cat_features = # list of categorical column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the transformers num_transformer for num_features\n",
    "num_transformer = # SimpleImputer\n",
    "\n",
    "# Create the transformers cat_transformer for cat_features\n",
    "cat_transformer = Pipeline(steps=[  \n",
    "    ('imputer', ), # SimpleImputer\n",
    "    ('onehot', ) # OneHotEncoder\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine transformers into a preprocessor with ColumnTransformer  \n",
    "preprocessor = ColumnTransformer(  \n",
    "    transformers=[  \n",
    "        ('num', num_transformer, num_features),  \n",
    "        ('cat', cat_transformer, cat_features)  \n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into features (X) and target variable (y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the preprocessed dataset into training and validation sets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Building a Pipeline:**\n",
    "   - Create a Scikit-learn pipeline that includes preprocessing steps (imputation, encoding) and a machine learning model.\n",
    "   - Choose a machine learning model (e.g., Logistic Regression, Decision Tree, Random Forest Classifier) and include it in the pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Scikit-learn pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the pipeline on the training data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Training and Evaluation:**\n",
    "   - Split the preprocessed dataset into training and validation sets.\n",
    "   - Fit the pipeline on the training data.\n",
    "   - Evaluate the pipeline on the validation data using accuracy score and classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the pipeline on the validation data\n",
    "\n",
    "print(f\"Accuracy Score: \")\n",
    "print(\"Classification Report:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **Parameter Tuning (Optional):**\n",
    "   - Experiment with different parameters of the pipeline components (e.g., model hyperparameters, imputation strategy).\n",
    "   - Use techniques like GridSearchCV to find the best combination of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid for GridSearchCV\n",
    "param_grid = {  \n",
    "    'model__n_estimators': [50, 100, 200],  \n",
    "    'model__max_depth': [None, 5, 10, 20],  \n",
    "    'preprocessor__num__strategy': ['mean', 'median']\n",
    "    # Add other parameters if desired\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the best parameters and best score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all of the results and sort by rank score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
