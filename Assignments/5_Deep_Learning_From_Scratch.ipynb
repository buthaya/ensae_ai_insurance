{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook has been inspired by the book from Sebastian Raschka \"Python Machine Learning\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Layer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going in this notebook to code a multi layer perceptron from scratch. A multi layer perceptron is a sequence of matrix computation, each followed by a non- linear function. In this notebook we are going to use as non linear function the sigmoid - even if currently ReLu is a more used function. We are going to show how to code a simple neural network.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 - Define a function that compute the sigmoid and plot the function\n",
    "\n",
    "The sigmoid is defined as:\n",
    "\n",
    "$$\n",
    "\\phi(z) = \\frac{1}{1+e^{-z}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    ## compute the sigmoid\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 - one hot function\n",
    "\n",
    "Write a function that takes in input a list of values and returns the one_hot values of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(y):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (one_hot([1,2]) == np.array([[1,0], [0., 1]]) ).all()\n",
    "assert (one_hot([2,1]) == np.array([[0,1], [1., 0]]) ).all()\n",
    "assert (one_hot([2, 1,2,3,1, 4]) == np.array([[0.,1.,0.,0.], [1,0.,0.,0.], [0.,1.,0., 0], [0, 0, 1, 0], [1.,0.,0.,0.], [0., 0., 0., 1.]])).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 - Cost function - logistic cost function\n",
    "\n",
    "We are going to use as cost function the logistic cost function (with no regularization). We are also going to normalize the logistic cost function with respect to the number of values we are considering, to be able to evaluate it on datasets of different lenghts.\n",
    "\n",
    "$$\n",
    "C =\\frac{1}{n} \\sum_{1}^{n}{-y^{i}*\\log(f(x^{i})+(1-y^{i}) *\\log(1-f(x^{i})}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "- n isthe number of datapoints\n",
    "- $y^{i}$ is the label for the $i^{th}$ element\n",
    "- f is the prediction function\n",
    "- $x^{i}$ is the $i^{th}$ element of the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(y_label, prediction):\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.abs(cost_function(np.array([[1,0]]), np.array([[0.9,0.1]])) - 0.21072103131565256) < 0.000001\n",
    "assert np.abs(cost_function(np.array([[1,0,], [0, 1]]), np.array([[0.999,0.1], [0.5, 0.5]])) - 0.7463276885556502) < 0.000001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Exercise 4 - Forward propagation \n",
    "\n",
    "In the following we are going to consider a multi-layer perceptron having only one hidden layer of a certain size. The non-linear function is going to be the sigmoid.\n",
    "\n",
    "Let X be data. The algorithm to compute the forward pass is the following:\n",
    "\n",
    "\n",
    "$\n",
    "z_{h} = X*w_{h} + b_{h} \\\\\n",
    "a_{h} = \\phi(z_{h}) \\\\\n",
    "z_{out} = z_{h}*w_{out} + b_{out} \\\\\n",
    "a_{out} = \\phi(z_out)\n",
    "$\n",
    "\n",
    "Function should return values for $z_{h}, a_{h}, z_{out}, a_{out}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X, w_h, b_h, w_out, b_out):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_X_test = [[1,0,1]]\n",
    "forward_wh_test = [[0,1], [1,0], [1,1]]\n",
    "forward_bh_test = [1,1]\n",
    "forward_wout_test = [[1,4,5], [2,-1, -1]]\n",
    "forward_bout_test = [1,2,-1]\n",
    "\n",
    "t1, t2, t3, t4 = forward(forward_X_test, forward_wh_test,forward_bh_test, forward_wout_test, forward_bout_test)\n",
    "\n",
    "assert (t1 == np.array([[2,3]])).all()\n",
    "assert (t2 - np.array([[0.88079708, 0.95257413]]) < 0.00001).all()\n",
    "assert (t3 - np.array([[3.78594533 ,4.57061419, 2.45141126]]) < 0.000001).all()\n",
    "assert (t4 - np.array([[0.99987661, 0.99908895, 0.99752738]]) < 0.00001).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5 - Back propagation\n",
    "\n",
    "We are going to need before being able to train a neural network, the back propagation step. In this step we are going to compute the error the model is doing with respect to the true label and backpropagate this information to every layer. Then we will update the weight of each layer with the information from the gradient.\n",
    "\n",
    "We are going to compute:\n",
    "\n",
    "$\n",
    "\\delta_{out} = a_{out} - y \\\\\n",
    "\\frac{\\partial \\phi(z_{h})}{\\partial z_{h}} = a_{h}*(1-a_{h}) \\\\\n",
    "\\delta_{h} = (\\delta_{out}*w_{out}^{T})*(a_{h}*(1-a_{h})) \\\\\n",
    "\\nabla(w_{h}) = X^{T}*\\delta_{h} \\\\\n",
    "\\nabla(b_{h}) = \\sum{\\delta_{h}} \\\\\n",
    "\\nabla(w_{out}) = a_{h}^{T}*\\delta_{out} \\\\\n",
    "\\nabla(b_{out}) = \\sum{\\delta_{out}}\n",
    "$\n",
    "\n",
    "It should return the values of\n",
    "\n",
    "$\n",
    "\\nabla(w_{h}) \\\\\n",
    "\\nabla(b_{h}) \\\\\n",
    "\\nabla(w_{out}) \\\\\n",
    "\\nabla(b_{out})\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(X, y, z_h, a_h, z_out, a_out, w_out):\n",
    "    pass\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g1, g2, g3, g4 = backpropagation(np.array(forward_X_test), [0,1,0], t1, t2, t3, t4, np.array(forward_wout_test))\n",
    "\n",
    "\n",
    "assert np.abs((g1 - np.array([[0.58168091, 0.04721922],\n",
    " [0.  ,       0.        ],\n",
    " [0.58168091, 0.04721922]]))).sum() < 0.0001\n",
    "assert np.abs((g2 - np.array([0.58168091, 0.04721922]))).sum() < 0.0001\n",
    "assert np.abs((g3 - np.array([[ 0.86125738, -0.00902424,  0.81091868],\n",
    " [ 0.93144212, -0.00975964,  0.87700127]]))).sum() < 0.0001\n",
    "assert np.abs((g4 - np.array([ 0.97781589, -0.01024554,  0.92066459]))).sum() < 0.0001\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 6 - Putting all together\n",
    "\n",
    "We are going to create a class called MultiLayerPerceptron that takes as input in the init:\n",
    "\n",
    "- the number of features of the dataset X $n_{features}$\n",
    "- the number of hidden neurons $n_{hidden}$\n",
    "- the number of output neurons $n_{output}$ (number of unique labels in target)\n",
    "\n",
    "We are going to initiate as class variables:\n",
    "- bias vector $b_{h}$ as a set of zeros of length $n_{hidden}$\n",
    "- matrix $w_{h}$ as a random normal matrix with size $(n_{features}, n_{hidden})$\n",
    "- bias vector $b_{out}$ as a set of zeros of length $n_{output}$\n",
    "- matrix $w_{out}$ as a random normal matrix with size $(n_{hidden}, n_{output})$\n",
    "\n",
    "Then we will integrate the two functions forward and backprop by using the class variables instead of having a static function. We are going to use both in the train method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "class MultiLayerPerceptron:\n",
    "    def __init__(self, n_features, n_hidden, n_output):\n",
    "        pass\n",
    "        \n",
    "    def forward(self, X):\n",
    "        pass\n",
    "   \n",
    "    \n",
    "    \n",
    "    def backprop(self, X_batch, y_batch, z_h, a_h, z_out, a_out):\n",
    "        pass\n",
    "\n",
    "    def train(self, X_train, y_train, X_val, y_val, epochs, batch_size, learning_rate):\n",
    "        epoch_loss_train = []\n",
    "        epoch_loss_val = []\n",
    "        batch_loss = []\n",
    "        \n",
    "        for idx_epoch in range(epochs):\n",
    "            # iterate over minibatches\n",
    "            indices = np.arange(X_train.shape[0])\n",
    "            batch_num = 1\n",
    "            for start_idx in range(0, indices.shape[0] - batch_size + 1, batch_size):\n",
    "                batch_idx = #select the good indices\n",
    "\n",
    "                X_batch = X_train[batch_idx]\n",
    "                y_batch = y_train[batch_idx]\n",
    "                \n",
    "                # forward propagation\n",
    "                z_h, a_h, z_out, a_out = None\n",
    "                \n",
    "                batch_cost = cost_function(y_batch, a_out)\n",
    "                batch_num += 1\n",
    "                \n",
    "                batch_loss.append(batch_cost)\n",
    "                grad_w_h, grad_b_h, grad_w_out, grad_b_out = None \n",
    "                \n",
    "                # Weight updates - do not forget the learning rate!\n",
    "                self.w_h\n",
    "                self.b_h  \n",
    "                self.w_out \n",
    "                self.b_out \n",
    "            \n",
    "            _, _, _, a_out = self.forward(X_train)\n",
    "            \n",
    "            cost = cost_function(y_train, a_out)\n",
    "            self.predict(X_val)\n",
    "            \n",
    "            _, _, _, a_out_val = self.forward(X_val)\n",
    "            cost_val = cost_function(y_val, a_out_val)\n",
    "            epoch_loss_train.append(cost)\n",
    "            epoch_loss_val.append(cost_val)\n",
    "            print(f'Epoch {idx_epoch}: train {cost} - eval {cost_val}')\n",
    "        return epoch_loss_train, epoch_loss_val, batch_loss\n",
    "    \n",
    "    def predict(self, X):\n",
    "        _, _, _, prediction = self.forward(X)\n",
    "        return np.argmax(prediction, axis=1)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        _, _, _, prediction = self.forward(X)\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the datasets\n",
    "\n",
    "We are going to use the digits dataset from sklearn. We will build a train, validation and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = digits.data\n",
    "y = one_hot(digits.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 7 - training a classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new object nn with the correct dimension for values $n_{features}$ and $n_{output}$. You can choose the number of hidden neurons you want to use.\n",
    "\n",
    "Call the *train* method with different epochs, batch_size and learning_rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_code = #instance class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_value, epoch_loss_val, batch_value = #train model with given values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 8 - compute accuracy metrics for train, test and valuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 9 - show examples of mislabelled images for training, valuation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 9 - plot training graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a graph having epoch loss for train and validation sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add to the plot a graph having epoch loss for batch loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using pytorch documentation, replicate what has been done from scratch.\n",
    "\n",
    "Change the definition of the network to see what works better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(64, 40),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(40,10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10,10)\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        logits = self.model(x)\n",
    "        return logits\n",
    "    \n",
    "       \n",
    "    def train(self, X_train, y_train, X_val, y_val, epochs, batch_size, learning_rate):\n",
    "        epoch_loss_train = []\n",
    "        epoch_loss_val = []\n",
    "        batch_loss = []\n",
    "        criterion = #use CrossEntropyLoss\n",
    "        optimizer = #choose an optim  \n",
    "        for idx_epoch in range(epochs):\n",
    "            # iterate over minibatches\n",
    "            indices = np.arange(X_train.shape[0])\n",
    "            batch_num = 1\n",
    "            for start_idx in range(0, indices.shape[0] - batch_size + 1, batch_size):\n",
    "                batch_idx = indices[start_idx:start_idx + batch_size]\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                X_batch = torch.from_numpy(X_train[batch_idx])\n",
    "                y_batch = torch.from_numpy(y_train[batch_idx])\n",
    "\n",
    "                outputs = #compute the output\n",
    "                loss = # compute the loss\n",
    "                \n",
    "                # compute the loss and optimize the code\n",
    "                \n",
    "                batch_loss.append(loss.item())\n",
    "            \n",
    "            # compute the loss for train and val\n",
    "            pred_train = \n",
    "            loss_train =\n",
    "            \n",
    "            pred_val = \n",
    "            loss_val = \n",
    "            print(f'Loss: {loss.item()} - {loss_val.item()}')\n",
    "            epoch_loss_train.append(loss_train.item() )\n",
    "            epoch_loss_val.append(loss_val.item() )\n",
    "\n",
    "        return epoch_loss_train, epoch_loss_val, batch_loss\n",
    "    \n",
    "    def compute_accuracy(self, X_values, y_values):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_nn = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train, loss_val, batch_loss = # train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 10 \n",
    "\n",
    "Evaluate the accuracy for train, evaluation and test set. Add a method compute_accuracy to the class NeuralNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 11\n",
    "\n",
    "Plot the graphs of training for both training and evaluation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 12 \n",
    "\n",
    "Find the set of images that are wrongly classified by both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 13 - Use a convolutional neuronal network instead of a linear network (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
